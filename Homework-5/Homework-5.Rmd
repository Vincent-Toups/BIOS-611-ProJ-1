---
title: "Homework 5"
author: "Matt Johnson"
date: "10/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gbm)
library(cluster)
library(ggfortify)
library(caret)
```

Q1
==

#### Read in data
```{r, message=F}
DF <- read_csv("https://raw.githubusercontent.com/Vincent-Toups/bios611-project1/master/source_data/datasets_26073_33239_weight-height.csv") 
write.csv(DF, "Data.csv")
```

#### Split up data
```{r}
DF$Gender = ifelse(DF$Gender == "Male", 1, 0) #Male is 1, female is 0

set.seed <- 18 #this is my lucky number 
spec = c(train = .6, test = .2, validate = .2)
DF1 = sample(cut(
  seq(nrow(DF)), 
  nrow(DF)*cumsum(c(0,spec)),
  labels = names(spec)
))

DF.Split = split(DF, DF1)
```

#### GBM
```{r, message=F}
gbm1 <- gbm(Gender ~ Height + Weight, data = DF.Split$train, distribution = "bernoulli")

DF.Split$validate$gbm1.probs <- predict(gbm1, newdata = DF.Split$validate, type = "response")
DF.Split$validate <- DF.Split$validate %>% 
  mutate(gbm1_pred = 1*(gbm1.probs > .5) + 0) %>% 
  mutate(accurate.gbm1 = 1*(gbm1_pred == Gender))
sum(DF.Split$validate$accurate.gbm1)/nrow(DF.Split$validate)
```

The Accuracy of the GBM model is about .9 on the validation set. This is much much better than the previous gbm that had an accuracy around .5

Q2
==

#### Read in data
```{r, message = F}
DF2 <- read_csv("https://raw.githubusercontent.com/Vincent-Toups/bios611-project1/master/source_data/datasets_38396_60978_charcters_stats.csv")
```

#### Q2-1
```{r}
summary(DF2) #nothing seems to out of the ordinary right here, lets make a couple plots

DF2 %>% 
  ggplot(aes(x = Alignment)) +
  geom_bar() #there are a lot of goods and not too many neutrals. Also seems to be a couple NAs, before modeling we should remove the NAs

DF2 <- na.omit(DF2)

#lets take a look at how many nuetrals there are
nrow(DF2 %>% filter(Alignment == "neutral"))
#since there are only 11 neutrals and we are mostly concerned with good vs bad lets just take these neutral people out

DF2.no.neut <- DF2 %>% 
  filter(Alignment != "neutral")

#lets check for some outliers here too
DF2.no.neut %>% 
  ggplot(aes(x = Intelligence)) +
  geom_bar(aes(color = Alignment), position = "dodge") #nothing really irregular here, seems like bad people are a bit more inteligent though which is interesting
#checked similar plots with all other qualities and did not find anything too out of the ordinary
```
I guess just taking out the 11 neutrals is all I am going to do in this section. This seems reasonable becasue there are not many of them and a person identified as "neutral" is not very interesting for the analysis. 

#### Q2-2
```{r}
pca <- prcomp(DF2.no.neut[, -c(1, 2)])
summary(pca) #we would just need the primary principle component since it accounts for 95% of the variance
round(pca$rot[,1],2) #how the 1st principle component is made up...seems to be mostly made of the total column
```

#### Q2-3
Since the columns we are interested in, the numerical ones exept total, are already on the same scale from 0-100 we should not need to normalize anything here. We could normalize everything if we wanted to include the total column, but we see above that we should probably not include this column.

#### Q2-4
```{r, eval = F}
Total.Test = rowSums(DF2.no.neut[, -c(1, 2, 9)])
DF2.no.neut$Total == Total.Test #seems like this is true
```
Yes it seems that the 'total' column is the total of the numeric columns

#### Q2-5
I don't think we should be using the 'total' column in the pca since it is made up of the other columns. You can see from the code in section Q2-3 that the primary component is mostly made up of the total column.

#### Q2-6
```{r}
autoplot(pca)
```
Seems like a random distribution to me, I wouldn't say there are any insights to be made from this plot. 

Q3
==
```{r}
DF.TSNE <- read.csv("TSNE.csv")
DF.TSNE %>% 
  ggplot(aes(x = X1, y = X2)) +
  geom_point(aes(color = cluster))
```

Insights, seems like there might be two groups, we should do a pca to really check. There is a bit of overlap, but it looks like there could be two groups with some outliers (Small men, big women).

Q4
==
Done in Home-work-5.ipynb

Q5
==
```{r}
DF2.no.neut$Alignment.bin <- ifelse(DF2.no.neut$Alignment == "good", 1, 0) #Good = 1, bad = 0

trainIndex <- createDataPartition(y = DF2.no.neut$Alignment.bin, p = .8, times = 1, list = F)
DF2.no.neut$Alignment.bin <- factor(DF2.no.neut$Alignment.bin)
train_ctrl <- trainControl(method = "repeatedcv", number = 50)
gbm2 <- train(Alignment.bin ~ Intelligence +
                Strength +
                Speed +
                Durability +
                Power +
                Combat +
                Total, data = DF2.no.neut %>% slice(trainIndex), 
              method = "gbm", 
              trControl = train_ctrl, 
              verbose = F)
# summary(gbm2)
gbm2
```
The Final Values used for the model are shown in the output above. 

Q6
==

Using things such as K-fold cross validation leads to a much more complete analysis. By taking many different samples and testing each individual we are able see how the model performs regardless of the sample taken. If we simply take one single sample we will not see the entire picture. We will only see how the model performs on that very specific set. Using CV better shows how the model will perform on data that is not in the training set, which in the end is the point of modeling. 

Q7
==

RFE works by starting with a full model and removes the least imporant features through fitting and refitting. RFE fits and test the full model and removes the feature that is the least important. It will then refit the model without this feature and test to remove another feature. The algorithm will continue like this until it finds the features that are most important. 
